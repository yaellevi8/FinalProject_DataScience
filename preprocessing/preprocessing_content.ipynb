{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d8629f",
   "metadata": {},
   "source": [
    "# Text Preprocessing:\n",
    "\n",
    "Data cleaning is an important and intensive process in Data science which aids in data analysis and building machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "46515d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unidecode \n",
    "import re\n",
    "import time \n",
    "import stopwords \n",
    "# nltk.download('stopwords') \n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n",
    "\n",
    "import langid\n",
    "from langdetect import DetectorFactory\n",
    "from langdetect import detect\n",
    "from langdetect import detect_langs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7db6c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_csv(url):\n",
    "    df = pd.read_csv(url)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e63481",
   "metadata": {},
   "source": [
    "# Now we start preparing the data:\n",
    "\n",
    "- Lowercase the text\n",
    "- Remove links\n",
    "- Remove non english language\n",
    "- remove punctuation, stop words\n",
    "\n",
    "https://github.com/kk7nc/Text_Classification\n",
    "\n",
    "# Stop words\n",
    "Text and document classification over social media, such as Twitter, Facebook, and so on is usually affected by the noisy nature (abbreviations, irregular forms) of the text corpuses.\n",
    "\n",
    "\n",
    "# Capitalization\n",
    "Sentences can contain a mixture of uppercase and lower case letters. Multiple sentences make up a text document. To reduce the problem space, the most common approach is to reduce everything to lower case. This brings all words in a document in same space, but it often changes the meaning of some words, such as \"US\" to \"us\" where first one represents the United States of America and second one is a pronoun. To solve this, slang and abbreviation converters can be applied.\n",
    "\n",
    "# Noise Removal\n",
    "Another issue of text cleaning as a pre-processing step is noise removal. Text documents generally contains characters like punctuations or special characters and they are not necessary for text mining or classification purposes. Although punctuation is critical to understand the meaning of the sentence, but it can affect the classification algorithms negatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "28b15e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_preprocessing_data_from_csv(data):  \n",
    "    cleaned_data = []\n",
    "    \n",
    "    for text in data:\n",
    "\n",
    "        # Replacing all the occurrences of \\n,\\\\n,\\t,\\\\ with a space.\n",
    "        formatted_text = text.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n",
    "\n",
    "        # Removing all the occurrences of links that starts with https\n",
    "        formatted_text = re.sub(r'http\\S+', '', formatted_text)\n",
    "\n",
    "        # Remove all the occurrences of text that ends with .com\n",
    "        formatted_text = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", formatted_text)\n",
    "\n",
    "        # Remove all whitespaces\n",
    "        pattern = re.compile(r'\\s+') \n",
    "        formatted_text = re.sub(pattern, ' ', formatted_text)\n",
    "        formatted_text = formatted_text.replace('?', ' ? ').replace(')', ') ')\n",
    "\n",
    "        # Remove accented characters from text using unidecode.\n",
    "        # Unidecode() - It takes unicode data & tries to represent it to ASCII characters. \n",
    "        remove_character = unidecode.unidecode(formatted_text)\n",
    "\n",
    "        # Convert text to lower case\n",
    "        lower_text = remove_character.lower()\n",
    "\n",
    "        # Pattern matching for all case alphabets\n",
    "        Pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
    "\n",
    "        # Limiting all the  repeatation to two characters.\n",
    "        formatted_text = Pattern_alpha.sub(r\"\\1\\1\", lower_text) \n",
    "\n",
    "        # Pattern matching for all the punctuations that can occur\n",
    "        Pattern_Punct = re.compile(r'(\\'[.,/#!\"$<>@[]^&%^&*?;:{}=_`~()+-])\\1{1,}')\n",
    "\n",
    "        # Limiting punctuations in previously formatted string to only one.\n",
    "        Combined_Formatted = Pattern_Punct.sub(r'\\1', formatted_text)\n",
    "\n",
    "        # The below statement is replacing repeatation of spaces that occur more than two times with that of one occurrence.\n",
    "        Final_Formatted = re.sub(' {2,}',' ', Combined_Formatted)\n",
    "\n",
    "        # The formatted text after removing not necessary punctuations.\n",
    "        Formatted_Text = re.sub(r\"[^a-zA-Z]+\", ' ', Final_Formatted) \n",
    "\n",
    "        # Text without stopwords\n",
    "        remove_stop_words = repr(Formatted_Text)\n",
    "        stoplist = stopwords.words('english') \n",
    "\n",
    "        # Append words to Medium.com\n",
    "        stoplist.extend(['ago', 'followers', 'pinned', 'read', 'min', 'published', 'days', 'hours', 'the'])\n",
    "            \n",
    "        No_StopWords = [word for word in word_tokenize(remove_stop_words) if word.lower() not in stoplist ]\n",
    "\n",
    "        # Convert list of tokens_without_stopwords to String type.\n",
    "        words_string = No_StopWords[0]\n",
    "        words_string = ' '.join(No_StopWords[1:]) \n",
    "        \n",
    "        # Remove more stop words  \n",
    "        final_text = remove_stopwords(words_string) \n",
    "        \n",
    "        # Split the \"'\" from the edges\n",
    "        cleaned_data.append(final_text[:len(final_text)-1])\n",
    "        \n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2d440d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_english_articles_and_remove_duplicated_rows(df):\n",
    "    # dropping ALL duplicate values (keep only one)\n",
    "    df.drop_duplicates('content', inplace = True)\n",
    "    \n",
    "    for topic in df[\"topic\"]:   \n",
    "        DetectorFactory.seed = 0\n",
    "        if detect(topic) != \"en\":\n",
    "            print(\"Found different language: \" + detect(topic))\n",
    "            df.drop(df.index[(df[\"topic\"] == topic)], axis=0, inplace=True)\n",
    "            \n",
    "    for content in df[\"content\"]:\n",
    "        DetectorFactory.seed = 0\n",
    "        if detect(content) != \"en\":\n",
    "            print(\"Found different language: \" + detect(content))\n",
    "            df.drop(df.index[(df[\"content\"] == content)], axis=0, inplace=True) #axis 0 for rows\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "98efef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows_with_short_content(df):\n",
    "    # also drop nulls\n",
    "    # split values by whitespace and drop data lt a word\n",
    "    df = df[df[\"topic\"].str.split().str.len() > 1]\n",
    "    print(str(df.shape))\n",
    "    # split values by whitespace and drop data lt a word and bt 5\n",
    "    df = df[df[\"category\"].str.split().str.len() < 5]\n",
    "    print(str(df.shape))\n",
    "    df = df[df[\"category\"].str.split().str.len() > 0]\n",
    "    print(str(df.shape))\n",
    "    # split values by whitespace and drop data lt 3 words \n",
    "    df = df[df[\"content\"].str.split().str.len() > 3]\n",
    "    print(str(df.shape))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20054a74",
   "metadata": {},
   "source": [
    "## Keep the 'cleaned' data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5089be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_df_to_csv(topic, category, content, csv_file_name):\n",
    "    new_df = pd.DataFrame({\n",
    "    \"topic\": topic,\n",
    "    \"category\": category,\n",
    "    \"content\": content\n",
    "    })\n",
    "    new_df.to_csv(csv_file_name)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "83cd78c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning data the shape of data frame is : (43954, 1218)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>topic</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 1208</th>\n",
       "      <th>Unnamed: 1209</th>\n",
       "      <th>Unnamed: 1210</th>\n",
       "      <th>Unnamed: 1211</th>\n",
       "      <th>Unnamed: 1212</th>\n",
       "      <th>Unnamed: 1213</th>\n",
       "      <th>Unnamed: 1214</th>\n",
       "      <th>Unnamed: 1215</th>\n",
       "      <th>Unnamed: 1216</th>\n",
       "      <th>Unnamed: 1217</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>The Importance the of Auditing Your Attention:...</td>\n",
       "      <td>business</td>\n",
       "      <td>1.3K Followers Pinned One of the most challen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Even the Best Startup Leaders Have This Weakne...</td>\n",
       "      <td>business</td>\n",
       "      <td>19.1K Followers Pinned All the actionable stu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Econ Made Me Jack Welch All My Friends</td>\n",
       "      <td>business</td>\n",
       "      <td>32 Followers 16 hours ago Or: How I learned t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10 Skills That’ll Make You Money (In 2022)</td>\n",
       "      <td>business</td>\n",
       "      <td>10.6K Followers Pinned How To Learn Faster By...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Your Biggest Risk Is You</td>\n",
       "      <td>business</td>\n",
       "      <td>647 Followers Published in Towards Data Scien...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43949</th>\n",
       "      <td>42136</td>\n",
       "      <td>10 Steps to Building a Data Collection System ...</td>\n",
       "      <td>technology</td>\n",
       "      <td>268 Followers Published in DataDrivenInvestor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43950</th>\n",
       "      <td>42137</td>\n",
       "      <td>When Should I Talk to My Child About Periods?</td>\n",
       "      <td>technology</td>\n",
       "      <td>1.99K Followers 23 hours ago Follow up questi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43951</th>\n",
       "      <td>42138</td>\n",
       "      <td>If I Did Things Differently</td>\n",
       "      <td>technology</td>\n",
       "      <td>23 Followers 22 hours ago Gaming has been wit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43952</th>\n",
       "      <td>42139</td>\n",
       "      <td>Get Paid Directly by Fans</td>\n",
       "      <td>technology</td>\n",
       "      <td>1.99K Followers Pinned Do at least 7 immediat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43953</th>\n",
       "      <td>42140</td>\n",
       "      <td>On Clarity vs Certainty: Strategy for Navigati...</td>\n",
       "      <td>technology</td>\n",
       "      <td>64 Followers Pinned On Clarity vs Certainty: ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43954 rows × 1218 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              topic  \\\n",
       "0              7  The Importance the of Auditing Your Attention:...   \n",
       "1              1  Even the Best Startup Leaders Have This Weakne...   \n",
       "2              2             Econ Made Me Jack Welch All My Friends   \n",
       "3              3         10 Skills That’ll Make You Money (In 2022)   \n",
       "4              4                           Your Biggest Risk Is You   \n",
       "...          ...                                                ...   \n",
       "43949      42136  10 Steps to Building a Data Collection System ...   \n",
       "43950      42137      When Should I Talk to My Child About Periods?   \n",
       "43951      42138                        If I Did Things Differently   \n",
       "43952      42139                          Get Paid Directly by Fans   \n",
       "43953      42140  On Clarity vs Certainty: Strategy for Navigati...   \n",
       "\n",
       "         category                                            content  \\\n",
       "0        business   1.3K Followers Pinned One of the most challen...   \n",
       "1        business   19.1K Followers Pinned All the actionable stu...   \n",
       "2        business   32 Followers 16 hours ago Or: How I learned t...   \n",
       "3        business   10.6K Followers Pinned How To Learn Faster By...   \n",
       "4        business   647 Followers Published in Towards Data Scien...   \n",
       "...           ...                                                ...   \n",
       "43949  technology   268 Followers Published in DataDrivenInvestor...   \n",
       "43950  technology   1.99K Followers 23 hours ago Follow up questi...   \n",
       "43951  technology   23 Followers 22 hours ago Gaming has been wit...   \n",
       "43952  technology   1.99K Followers Pinned Do at least 7 immediat...   \n",
       "43953  technology   64 Followers Pinned On Clarity vs Certainty: ...   \n",
       "\n",
       "      Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9  ...  \\\n",
       "0            NaN        NaN        NaN        NaN        NaN        NaN  ...   \n",
       "1            NaN        NaN        NaN        NaN        NaN        NaN  ...   \n",
       "2            NaN        NaN        NaN        NaN        NaN        NaN  ...   \n",
       "3            NaN        NaN        NaN        NaN        NaN        NaN  ...   \n",
       "4            NaN        NaN        NaN        NaN        NaN        NaN  ...   \n",
       "...          ...        ...        ...        ...        ...        ...  ...   \n",
       "43949        NaN        NaN        NaN        NaN        NaN        NaN  ...   \n",
       "43950        NaN        NaN        NaN        NaN        NaN        NaN  ...   \n",
       "43951        NaN        NaN        NaN        NaN        NaN        NaN  ...   \n",
       "43952        NaN        NaN        NaN        NaN        NaN        NaN  ...   \n",
       "43953        NaN        NaN        NaN        NaN        NaN        NaN  ...   \n",
       "\n",
       "      Unnamed: 1208 Unnamed: 1209 Unnamed: 1210 Unnamed: 1211 Unnamed: 1212  \\\n",
       "0               NaN           NaN           NaN           NaN           NaN   \n",
       "1               NaN           NaN           NaN           NaN           NaN   \n",
       "2               NaN           NaN           NaN           NaN           NaN   \n",
       "3               NaN           NaN           NaN           NaN           NaN   \n",
       "4               NaN           NaN           NaN           NaN           NaN   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "43949           NaN           NaN           NaN           NaN           NaN   \n",
       "43950           NaN           NaN           NaN           NaN           NaN   \n",
       "43951           NaN           NaN           NaN           NaN           NaN   \n",
       "43952           NaN           NaN           NaN           NaN           NaN   \n",
       "43953           NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "      Unnamed: 1213 Unnamed: 1214 Unnamed: 1215 Unnamed: 1216 Unnamed: 1217  \n",
       "0               NaN           NaN           NaN           NaN           NaN  \n",
       "1               NaN           NaN           NaN           NaN           NaN  \n",
       "2               NaN           NaN           NaN           NaN           NaN  \n",
       "3               NaN           NaN           NaN           NaN           NaN  \n",
       "4               NaN           NaN           NaN           NaN           NaN  \n",
       "...             ...           ...           ...           ...           ...  \n",
       "43949           NaN           NaN           NaN           NaN           NaN  \n",
       "43950           NaN           NaN           NaN           NaN           NaN  \n",
       "43951           NaN           NaN           NaN           NaN           NaN  \n",
       "43952           NaN           NaN           NaN           NaN           NaN  \n",
       "43953           NaN           NaN           NaN           NaN           NaN  \n",
       "\n",
       "[43954 rows x 1218 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_data_from_csv(\"merge.csv\")\n",
    "print(\"Before cleaning data the shape of data frame is : \" + str(df.shape))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6d88a507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42946, 1218)\n",
      "(42817, 1218)\n",
      "(42817, 1218)\n",
      "(42575, 1218)\n",
      "After drop_rows_with_short_content the shape of data frame is : (42575, 1218)\n",
      "Found different language: de\n",
      "Found different language: no\n",
      "Found different language: nl\n",
      "Found different language: de\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6568/2220443692.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"After drop_rows_with_short_content the shape of data frame is : \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_non_english_articles_and_remove_duplicated_rows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"After remove_non_english_articles_and_remove_duplicated_rows the shape of data frame is : \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6568/1241069490.py\u001b[0m in \u001b[0;36mremove_non_english_articles_and_remove_duplicated_rows\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"en\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Found different language: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"topic\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtopic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcontent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"content\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4904\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4905\u001b[0m         \"\"\"\n\u001b[1;32m-> 4906\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4907\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4908\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4152\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4153\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4154\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4155\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_update_inplace\u001b[1;34m(self, result, verify_is_copy)\u001b[0m\n\u001b[0;32m   4236\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4237\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4238\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4239\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverify_is_copy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_is_copy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   5498\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5499\u001b[0m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5500\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5501\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5502\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = drop_rows_with_short_content(df)\n",
    "print(\"After drop_rows_with_short_content the shape of data frame is : \" + str(df.shape))\n",
    "\n",
    "df = remove_non_english_articles_and_remove_duplicated_rows(df)\n",
    "print(\"After remove_non_english_articles_and_remove_duplicated_rows the shape of data frame is : \" + str(df.shape))\n",
    "\n",
    "title_list = cleaning_preprocessing_data_from_csv(df[\"topic\"])\n",
    "content_list = cleaning_preprocessing_data_from_csv(df[\"content\"])\n",
    "\n",
    "new_df = insert_df_to_csv(title_list, df[\"category\"], content_list, \"cleaned-scraping-data.csv\")\n",
    "print(\"After cleaning data the shape of data frame is : \" + str(new_df.shape))\n",
    "new_df"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d8629f",
   "metadata": {},
   "source": [
    "# Text Preprocessing:\n",
    "\n",
    "Data cleaning is an important and intensive process in Data science which aids in data analysis and building machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46515d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unidecode \n",
    "import re\n",
    "import time \n",
    "import stopwords \n",
    "# nltk.download('stopwords') \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n",
    "\n",
    "import langid\n",
    "from langdetect import DetectorFactory\n",
    "from langdetect import detect\n",
    "from langdetect import detect_langs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7db6c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_csv(url):\n",
    "    df = pd.read_csv(url)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e63481",
   "metadata": {},
   "source": [
    "# Now we start preparing the data:\n",
    "\n",
    "- Lowercase the text\n",
    "- Remove links\n",
    "- Remove non english language\n",
    "- remove punctuation, stop words\n",
    "\n",
    "https://github.com/kk7nc/Text_Classification\n",
    "\n",
    "# Stop words\n",
    "Text and document classification over social media, such as Twitter, Facebook, and so on is usually affected by the noisy nature (abbreviations, irregular forms) of the text corpuses.\n",
    "\n",
    "\n",
    "# Capitalization\n",
    "Sentences can contain a mixture of uppercase and lower case letters. Multiple sentences make up a text document. To reduce the problem space, the most common approach is to reduce everything to lower case. This brings all words in a document in same space, but it often changes the meaning of some words, such as \"US\" to \"us\" where first one represents the United States of America and second one is a pronoun. To solve this, slang and abbreviation converters can be applied.\n",
    "\n",
    "# Noise Removal\n",
    "Another issue of text cleaning as a pre-processing step is noise removal. Text documents generally contains characters like punctuations or special characters and they are not necessary for text mining or classification purposes. Although punctuation is critical to understand the meaning of the sentence, but it can affect the classification algorithms negatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28b15e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_preprocessing_data_from_csv(data):  \n",
    "    cleaned_data = []\n",
    "    \n",
    "    for text in data:\n",
    "\n",
    "        # Replacing all the occurrences of \\n,\\\\n,\\t,\\\\ with a space.\n",
    "        formatted_text = text.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n",
    "\n",
    "        # Removing all the occurrences of links that starts with https\n",
    "        formatted_text = re.sub(r'http\\S+', '', formatted_text)\n",
    "\n",
    "        # Remove all the occurrences of text that ends with .com\n",
    "        formatted_text = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", formatted_text)\n",
    "\n",
    "        # Remove all whitespaces\n",
    "        pattern = re.compile(r'\\s+') \n",
    "        formatted_text = re.sub(pattern, ' ', formatted_text)\n",
    "        formatted_text = formatted_text.replace('?', ' ? ').replace(')', ') ')\n",
    "\n",
    "        # Remove accented characters from text using unidecode.\n",
    "        # Unidecode() - It takes unicode data & tries to represent it to ASCII characters. \n",
    "        remove_character = unidecode.unidecode(formatted_text)\n",
    "\n",
    "        # Convert text to lower case\n",
    "        lower_text = remove_character.lower()\n",
    "\n",
    "        # Pattern matching for all case alphabets\n",
    "        Pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
    "\n",
    "        # Limiting all the  repeatation to two characters.\n",
    "        formatted_text = Pattern_alpha.sub(r\"\\1\\1\", lower_text) \n",
    "\n",
    "        # Pattern matching for all the punctuations that can occur\n",
    "        Pattern_Punct = re.compile(r'(\\'[.,/#!\"$<>@[]^&%^&*?;:{}=_`~()+-])\\1{1,}')\n",
    "\n",
    "        # Limiting punctuations in previously formatted string to only one.\n",
    "        Combined_Formatted = Pattern_Punct.sub(r'\\1', formatted_text)\n",
    "\n",
    "        # The below statement is replacing repeatation of spaces that occur more than two times with that of one occurrence.\n",
    "        Final_Formatted = re.sub(' {2,}',' ', Combined_Formatted)\n",
    "\n",
    "        # The formatted text after removing not necessary punctuations.\n",
    "        Formatted_Text = re.sub(r\"[^a-zA-Z]+\", ' ', Final_Formatted) \n",
    "        \n",
    "#         # Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item\n",
    "#         wordnet = WordNetLemmatizer()\n",
    "#         Formatted_Text =  \" \".join([wordnet.lemmatize(word) for word in Formatted_Text])\n",
    "#         print(\"Finish lemmatizer\")\n",
    "\n",
    "        # Text without stopwords\n",
    "        remove_stop_words = repr(Formatted_Text)\n",
    "        stoplist = stopwords.words('english') \n",
    "\n",
    "        # Append words to Medium.com\n",
    "        stoplist.extend(['ago', 'followers', 'pinned', 'read', 'min', 'published', 'days', 'hours', 'the'])\n",
    "            \n",
    "        No_StopWords = [word for word in word_tokenize(remove_stop_words) if word.lower() not in stoplist ]\n",
    "\n",
    "        # Convert list of tokens_without_stopwords to String type.\n",
    "        words_string = No_StopWords[0]\n",
    "        words_string = ' '.join(No_StopWords[1:]) \n",
    "        \n",
    "        # Remove more stop words  \n",
    "        final_text = remove_stopwords(words_string) \n",
    "        \n",
    "        # Split the \"'\" from the edges\n",
    "        cleaned_data.append(final_text[:len(final_text)-1])\n",
    "        \n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d440d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_english_articles_and_remove_duplicated_rows(df):\n",
    "    # dropping ALL duplicate values (keep only one)\n",
    "    df.drop_duplicates('content', inplace = True)\n",
    "    \n",
    "#     print(\"DETECT TOPICS:\")\n",
    "#     for topic in df[\"topic\"]:\n",
    "#         DetectorFactory.seed = 0\n",
    "#         if detect(topic) != \"en\":\n",
    "#             print(topic)\n",
    "#             print(\"Found different language: \" + detect(topic))\n",
    "#             df.drop(df.index[(df[\"topic\"] == topic)], axis=0, inplace=True)\n",
    "    \n",
    "#     print(\"DETECT CONTENTS:\")\n",
    "#     for content in df[\"content\"]:\n",
    "#         DetectorFactory.seed = 0\n",
    "#         if detect(content) != \"en\":\n",
    "#             print(\"Found different language: \" + detect(content))\n",
    "#             df.drop(df.index[(df[\"content\"] == content)], axis=0, inplace=True) #axis 0 for rows\n",
    "    \n",
    "#     print(\"DETECT CATEGORIES:\")\n",
    "#     for category in df[\"category\"]:   \n",
    "#         DetectorFactory.seed = 0\n",
    "#         if detect(topic) != \"en\":\n",
    "#             print(category)\n",
    "#             print(\"Found different language: \" + detect(topic))\n",
    "#             df.drop(df.index[(df[\"category\"] == category)], axis=0, inplace=True)\n",
    "            \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98efef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows_with_short_content(df):\n",
    "    # also drop nulls\n",
    "    # split values by whitespace and drop data lt a word\n",
    "    df = df[df[\"topic\"].str.split().str.len() > 1]\n",
    "    \n",
    "    # split values by whitespace and drop data lt a word and bt 5\n",
    "    df = df[df[\"category\"].str.split().str.len() < 5]\n",
    "    df = df[df[\"category\"].str.split().str.len() > 0]\n",
    "\n",
    "    # split values by whitespace and drop data lt 3 words \n",
    "    df = df[df[\"content\"].str.split().str.len() > 3]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20054a74",
   "metadata": {},
   "source": [
    "## Keep the 'cleaned' data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5089be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_df_to_csv(topic, category, content, csv_file_name):\n",
    "    new_df = pd.DataFrame({\n",
    "#     \"topic\": topic,\n",
    "    \"category\": category,\n",
    "    \"content\": content\n",
    "    })\n",
    "    new_df.to_csv(csv_file_name)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83cd78c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning data the shape of data frame is : (1490, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>857</td>\n",
       "      <td>double eviction from big brother model caprice...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>325</td>\n",
       "      <td>dj double act revamp chart show dj duo jk and ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>1590</td>\n",
       "      <td>weak dollar hits reuters revenues at media gro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>1587</td>\n",
       "      <td>apple ipod family expands market apple has exp...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>538</td>\n",
       "      <td>santy worm makes unwelcome visit thousands of ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1490 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                            content  \\\n",
       "0           1833  worldcom ex-boss launches defence lawyers defe...   \n",
       "1            154  german business confidence slides german busin...   \n",
       "2           1101  bbc poll indicates economic gloom citizens in ...   \n",
       "3           1976  lifestyle  governs mobile choice  faster  bett...   \n",
       "4            917  enron bosses in $168m payout eighteen former e...   \n",
       "...          ...                                                ...   \n",
       "1485         857  double eviction from big brother model caprice...   \n",
       "1486         325  dj double act revamp chart show dj duo jk and ...   \n",
       "1487        1590  weak dollar hits reuters revenues at media gro...   \n",
       "1488        1587  apple ipod family expands market apple has exp...   \n",
       "1489         538  santy worm makes unwelcome visit thousands of ...   \n",
       "\n",
       "           category  \n",
       "0          business  \n",
       "1          business  \n",
       "2          business  \n",
       "3              tech  \n",
       "4          business  \n",
       "...             ...  \n",
       "1485  entertainment  \n",
       "1486  entertainment  \n",
       "1487       business  \n",
       "1488           tech  \n",
       "1489           tech  \n",
       "\n",
       "[1490 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_data_from_csv(\"BBC-News-Train.csv\")\n",
    "print(\"Before cleaning data the shape of data frame is : \" + str(df.shape))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d88a507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After drop_rows_with_short_content the shape of data frame is : (1490, 3)\n",
      "After remove_non_english_articles_and_remove_duplicated_rows the shape of data frame is : (1440, 3)\n",
      "Finish cleaning on content\n"
     ]
    }
   ],
   "source": [
    "df = drop_rows_with_short_content(df)\n",
    "print(\"After drop_rows_with_short_content the shape of data frame is : \" + str(df.shape))\n",
    "\n",
    "df = remove_non_english_articles_and_remove_duplicated_rows(df)\n",
    "print(\"After remove_non_english_articles_and_remove_duplicated_rows the shape of data frame is : \" + str(df.shape))\n",
    "\n",
    "# def lemmatize_word(text):\n",
    "#     wordnet = WordNetLemmatizer()\n",
    "#     return \" \".join([wordnet.lemmatize(word) for word in text])\n",
    "# df['content'] = df['content'].apply(lemmatize_word)\n",
    "# df\n",
    "title_list = cleaning_preprocessing_data_from_csv(df[\"topic\"])\n",
    "print(\"Finish cleaning on topic\")\n",
    "content_list = cleaning_preprocessing_data_from_csv(df[\"content\"])\n",
    "print(\"Finish cleaning on content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "572062a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_category_and_union_categories_with_same_meaning():\n",
    "    category_list = []\n",
    "    for category in df[\"category\"]:\n",
    "        lower_category = category.lower()\n",
    "        if lower_category == \"sports\":\n",
    "            lower_category = \"sport\"\n",
    "        if lower_category == \"health-and-fitness\":\n",
    "            lower_category = \"health-fitness\"\n",
    "        if lower_category == \"arts-and-entertainment\":\n",
    "            lower_category = \"arts-entertainment\"\n",
    "        if lower_category == \"computers-and-technology\" or lower_category == \"computers-technology\":\n",
    "            lower_category = \"computers\"\n",
    "        if lower_category == \"travel-and-leisure\" or lower_category == \"travel-leisure\" or lower_category == \"travel-and-tourism-lifestyle-and-leisure\":\n",
    "            lower_category = \"travel\"\n",
    "        if lower_category == \"news-and-society\" or lower_category == \"news-society\":\n",
    "            lower_category = \"society\"\n",
    "        if lower_category == \"food-and-drink\":\n",
    "            lower_category = \"food\"\n",
    "        category_list.append(lower_category)\n",
    "\n",
    "    print(len(category_list))\n",
    "    return category_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f0dbca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440\n",
      "After cleaning data the shape of data frame is : (1440, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>ex boss launches defence lawyers defending wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>business confidence slides german business con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>poll indicates economic gloom citizens majorit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tech</td>\n",
       "      <td>governs mobile choice faster better funkier ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>bosses payout eighteen enron directors agreed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>eviction big brother model caprice holby city ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>double act revamp chart dj duo jk joel taking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>business</td>\n",
       "      <td>dollar hits reuters revenues media group reute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>tech</td>\n",
       "      <td>ipod family expands market apple expanded ipod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>tech</td>\n",
       "      <td>worm makes unwelcome visit thousands website b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1440 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           category                                            content\n",
       "0          business  ex boss launches defence lawyers defending wor...\n",
       "1          business  business confidence slides german business con...\n",
       "2          business  poll indicates economic gloom citizens majorit...\n",
       "3              tech  governs mobile choice faster better funkier ha...\n",
       "4          business  bosses payout eighteen enron directors agreed ...\n",
       "...             ...                                                ...\n",
       "1435  entertainment  eviction big brother model caprice holby city ...\n",
       "1436  entertainment  double act revamp chart dj duo jk joel taking ...\n",
       "1437       business  dollar hits reuters revenues media group reute...\n",
       "1438           tech  ipod family expands market apple expanded ipod...\n",
       "1439           tech  worm makes unwelcome visit thousands website b...\n",
       "\n",
       "[1440 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_list = lower_category_and_union_categories_with_same_meaning()\n",
    "new_df = insert_df_to_csv(title_list, category_list, content_list, \"cleaned-scraping-data.csv\")\n",
    "print(\"After cleaning data the shape of data frame is : \" + str(new_df.shape))\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96c23ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sport            342\n",
      "business         335\n",
      "politics         266\n",
      "entertainment    263\n",
      "tech             234\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(new_df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72153f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e72fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import time\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup  \n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e61fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_html(url):\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content,\"html.parser\")\n",
    "    time.sleep(5) \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f314dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_articlebiz_web(soup):\n",
    "    \n",
    "    articles = soup.find_all('div',{\"class\": lambda L: L and L.startswith('mb-4 card')})\n",
    "    \n",
    "    # Declaration\n",
    "    articles_topic = []\n",
    "    articles_content = []\n",
    "\n",
    "    for article in articles:\n",
    "        # Get article's topic\n",
    "        topic = article.find(\"a\").get_text()\n",
    "        articles_topic.append(topic)\n",
    "\n",
    "        # Get article's content\n",
    "       \n",
    "        url_of_article = article.find('a')['href']\n",
    "\n",
    "        soup = get_article_html(url_of_article)\n",
    "\n",
    "        content = soup.find('div',attrs={'class':'clearfix'}).find_all(\"p\")\n",
    "        full_content = \"\"\n",
    "        if content != None:\n",
    "            for paragraph in content:\n",
    "                full_content = full_content + \" \" + paragraph.get_text().strip()\n",
    "        articles_content.append(full_content) \n",
    "        \n",
    "    return articles_topic, articles_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dd7d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_to_csv(topic, category, content):\n",
    "    # Create dataframe\n",
    "    data = pd.DataFrame({\n",
    "    \"topic\": topic,\n",
    "    \"category\": category,\n",
    "    \"content\": content\n",
    "    })\n",
    "    data.to_csv(f'articlebiz Scraping-{category}-{datetime.now().strftime(\"%d-%m-%Y\")}.csv', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614d87cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"politics\", \"business\", \"pets\", \"news-society\", \"arts-entertainment\", \"travel-leisure\", \"social-issues\", \"autos-trucks\", \"health-fitness\",\n",
    "              \"shopping\", \"computers-technology\", \"finance\", \"home\", \"sports-recreations\"]\n",
    "\n",
    "print(\"Start scraping from articlebiz!\")\n",
    "\n",
    "for category in categories:\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(f\"https://articlebiz.com/category/{category}\")\n",
    "    driver.maximize_window()\n",
    "    time.sleep(3)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    topic, content = get_data_articlebiz_web(soup)\n",
    "    insert_data_to_csv(topic, category, content)\n",
    "    next_button = driver.find_element(by=By.LINK_TEXT, value=('Next')).click() \n",
    "    for i in range (1, 28):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(10)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "        topic, content = get_data_articlebiz_web(soup)\n",
    "        insert_data_to_csv(topic, category, content)\n",
    "        next_button = driver.find_element(by=By.LINK_TEXT, value=('Next')).click() \n",
    "    driver.close()\n",
    "    \n",
    "print(\"Finish scraping from articlebiz.com!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

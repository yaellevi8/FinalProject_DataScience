{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e72fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import time\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup  \n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e61fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_html(url):\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content,\"html.parser\")\n",
    "    time.sleep(5) \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f314dbf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_data_abc_web(soup):   \n",
    "    all_articles_part1= soup.find_all('div',attrs={'class':'yqrQw'})\n",
    "    all_articles_part2 = soup.find_all('div',attrs={'class':'_2I1aj'})\n",
    "    \n",
    "    # Declaration\n",
    "    articles_topic = []\n",
    "    articles_content = []\n",
    "   \n",
    "    for article in all_articles_part1:\n",
    "        topic = article.find('a').get_text()\n",
    "        articles_topic.append(topic)\n",
    "\n",
    "        url_of_article = article.find('a')['href']\n",
    "        soup = get_article_html(\"https://www.abc.net.au/\" + url_of_article)\n",
    "\n",
    "        content = soup.find_all(\"p\")\n",
    "        full_content = \"\"\n",
    "        if content != None:\n",
    "            for paragraph in content:\n",
    "                full_content = full_content + \" \" + paragraph.get_text().strip()\n",
    "        articles_content.append(full_content) \n",
    "        \n",
    "    for article in all_articles_part2:\n",
    "        if article.find('a') != None:\n",
    "            topic = article.find('a').get_text()\n",
    "        \n",
    "            if article.find('a')['href'] != None:\n",
    "                url_of_article = article.find('a')['href']\n",
    "                soup = get_article_html(\"https://www.abc.net.au/\" + url_of_article)\n",
    "\n",
    "                content = soup.find_all(\"p\")\n",
    "                full_content = \"\"\n",
    "                if content != None:\n",
    "                    for paragraph in content:\n",
    "                        full_content = full_content + \" \" + paragraph.get_text().strip()\n",
    "                articles_topic.append(topic)\n",
    "                articles_content.append(full_content) \n",
    "      \n",
    "    return articles_topic, articles_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dd7d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_to_csv(topic, category, content):\n",
    "    # Create dataframe\n",
    "    data = pd.DataFrame({\n",
    "    \"topic\": topic,\n",
    "    \"category\": category,\n",
    "    \"content\": content\n",
    "    })\n",
    "    data.to_csv(f'abc Scraping-{category}-{datetime.now().strftime(\"%d-%m-%Y\")}.csv', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd96e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"politics\", \"analysis-and-opinion\", \"arts-culture\", \"environment\", \"house-and-home\",\n",
    "              \"travel-and-tourism-lifestyle-and-leisure\", \"shopping-mall\", \"markets\", \"society\"\n",
    "              \"business\", \"world\", \"health\", \"sport\", \"science\",]\n",
    "\n",
    "print(\"Start scraping from abc!\")\n",
    "\n",
    "def scraping_with_selenium_from_url(url):\n",
    "        driver = webdriver.Chrome()        \n",
    "        driver.get(url)\n",
    "        driver.maximize_window()\n",
    "        time.sleep(5)\n",
    "        close_cookies= \"//button[@class='_1KwgR _1uCkA _2vzFN _20SSK eCXal _2VqCY pigdr']\"\n",
    "        driver.find_element(by=By.XPATH, value=close_cookies).click()\n",
    "        time.sleep(5)\n",
    "        for i in range (1, 20):\n",
    "            next_xpath = \"//button[@class='_1KwgR _2-5J1 eCXal']\"\n",
    "            driver.find_element(by=By.XPATH, value=next_xpath).click() \n",
    "            time.sleep(3)\n",
    "        html = driver.page_source\n",
    "        driver.close()\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "        topic, content = get_data_abc_web(soup)\n",
    "        insert_data_to_csv(topic, category, content)\n",
    "    \n",
    "for category in categories:\n",
    "    if category in (\"environment\", \"house-and-home\", \"travel-and-tourism-lifestyle-and-leisure\", \"shopping-mall\",  \"markets\", \"society\"):\n",
    "        scraping_with_selenium_from_url(f\"https://www.abc.net.au/news/topic/{category}\")\n",
    "    else:\n",
    "        scraping_with_selenium_from_url(f\"https://www.abc.net.au/news/{category}\")\n",
    "        \n",
    "print(\"Finish scraping from abc.net.au!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c70176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

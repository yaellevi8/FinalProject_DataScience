{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d8629f",
   "metadata": {},
   "source": [
    "# Cleaning data from csv\n",
    "\n",
    "Data cleaning is an important and intensive process in Data science which aids in data analysis and building machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "id": "46515d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import unidecode \n",
    "import re\n",
    "import time \n",
    "import stopwords \n",
    "nltk.download('stopwords') \n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n",
    "import langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "id": "7db6c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_csv(url):\n",
    "    return pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "id": "28b15e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_preprocessing_data_from_csv(data):  \n",
    "    cleaned_data = []\n",
    "    \n",
    "    for text in data:\n",
    "\n",
    "        # Replacing all the occurrences of \\n,\\\\n,\\t,\\\\ with a space.\n",
    "        formatted_text = text.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n",
    "\n",
    "        # Removing all the occurrences of links that starts with https\n",
    "        formatted_text = re.sub(r'http\\S+', '', formatted_text)\n",
    "\n",
    "        # Remove all the occurrences of text that ends with .com\n",
    "        formatted_text = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", formatted_text)\n",
    "\n",
    "        # Remove all whitespaces\n",
    "        pattern = re.compile(r'\\s+') \n",
    "        formatted_text = re.sub(pattern, ' ', formatted_text)\n",
    "        formatted_text = formatted_text.replace('?', ' ? ').replace(')', ') ')\n",
    "\n",
    "        # Remove accented characters from text using unidecode.\n",
    "        # Unidecode() - It takes unicode data & tries to represent it to ASCII characters. \n",
    "        remove_character = unidecode.unidecode(formatted_text)\n",
    "\n",
    "        # Convert text to lower case\n",
    "        lower_text = remove_character.lower()\n",
    "\n",
    "        # Pattern matching for all case alphabets\n",
    "        Pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
    "\n",
    "        # Limiting all the  repeatation to two characters.\n",
    "        formatted_text = Pattern_alpha.sub(r\"\\1\\1\", lower_text) \n",
    "\n",
    "        # Pattern matching for all the punctuations that can occur\n",
    "        Pattern_Punct = re.compile(r'(\\'[.,/#!\"$<>@[]^&%^&*?;:{}=_`~()+-])\\1{1,}')\n",
    "\n",
    "        # Limiting punctuations in previously formatted string to only one.\n",
    "        Combined_Formatted = Pattern_Punct.sub(r'\\1', formatted_text)\n",
    "\n",
    "        # The below statement is replacing repeatation of spaces that occur more than two times with that of one occurrence.\n",
    "        Final_Formatted = re.sub(' {2,}',' ', Combined_Formatted)\n",
    "\n",
    "        # The formatted text after removing not necessary punctuations.\n",
    "        Formatted_Text = re.sub(r\"[^a-zA-Z]+\", ' ', Final_Formatted) \n",
    "\n",
    "        # Text without stopwords\n",
    "        remove_stop_words = repr(Formatted_Text)\n",
    "        stoplist = stopwords.words('english') \n",
    "        stoplist = set(stoplist)\n",
    "        No_StopWords = [word for word in word_tokenize(remove_stop_words) if word.lower() not in stoplist ]\n",
    "\n",
    "        # Convert list of tokens_without_stopwords to String type.\n",
    "        words_string = ' '.join(No_StopWords) \n",
    "\n",
    "        # Remove more stop words  \n",
    "        final_text = remove_stopwords(words_string) \n",
    "        \n",
    "        # Split the \"'\" from the edges\n",
    "        cleaned_data.append(final_text[1:len(final_text)-1])\n",
    "        \n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "id": "2d440d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_english_articles_and_remove_duplicated_rows(df):\n",
    "    # Remove duplicated rows\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    for topic in df[\"topic\"]:   \n",
    "        DetectorFactory.seed = 0\n",
    "        if detect(topic) != \"en\":\n",
    "            print(\"Found different language: \" + detect(topic))\n",
    "            df.drop(df.index[(df[\"topic\"] == topic)], axis=0, inplace=True)\n",
    "            \n",
    "    for content in df[\"content\"]:\n",
    "        DetectorFactory.seed = 0\n",
    "        if detect(content) != \"en\":\n",
    "            print(\"Found different language: \" + detect(content))\n",
    "            df.drop(df.index[(df[\"content\"] == content)], axis=0, inplace=True)        \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "id": "5089be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_df_to_csv(topic, category, content, csv_file_name):\n",
    "    data = pd.DataFrame({\n",
    "    \"topic\": topic,\n",
    "    \"category\": category,\n",
    "    \"content\": content\n",
    "    })\n",
    "    data.to_csv(csv_file_name)\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "id": "83cd78c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Cleaning Data!\n",
      "Found different language: de\n",
      "Found different language: no\n",
      "Found different language: nl\n",
      "(21, 4)\n",
      "Finish Cleaning Data!\n"
     ]
    }
   ],
   "source": [
    "print(\"Start Cleaning Data!\")\n",
    "df = get_data_from_csv(\"example.csv\")\n",
    "df = remove_non_english_articles_and_remove_duplicated_rows(df)\n",
    "title_list = cleaning_preprocessing_data_from_csv(df[\"topic\"])\n",
    "content_list = cleaning_preprocessing_data_from_csv(df[\"content\"])\n",
    "insert_df_to_csv(title_list, df[\"category\"], content_list, \"cleaned-scraping-data.csv\")\n",
    "print(\"Finish Cleaning Data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b706353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
